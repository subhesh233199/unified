@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
async def run_full_analysis(request: FolderPathRequest) -> AnalysisResponse:
    folder_path = convert_windows_path(request.folder_path)
    folder_path = os.path.normpath(folder_path)
   
    if not os.path.exists(folder_path):
        raise HTTPException(status_code=400, detail=f"Folder path does not exist: {folder_path}")
   
    pdf_files = get_pdf_files_from_folder(folder_path)
    logger.info(f"Processing {len(pdf_files)} PDF files")

    versions = []
    for pdf_path in pdf_files:
        match = re.search(r'(\d+\.\d+)(?:\s|\.)', os.path.basename(pdf_path))
        if match:
            versions.append(match.group(1))
    versions = sorted(set(versions))
    if len(versions) < 2:
        raise HTTPException(status_code=400, detail="At least two versions are required for analysis")

    # Parallel PDF processing
    extracted_texts = []
    all_hyperlinks = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        text_futures = {executor.submit(extract_text_from_pdf, pdf): pdf for pdf in pdf_files}
        hyperlink_futures = {executor.submit(extract_hyperlinks_from_pdf, pdf): pdf for pdf in pdf_files}
       
        for future in as_completed(text_futures):
            pdf = text_futures[future]
            try:
                text = locate_table(future.result(), START_HEADER_PATTERN, END_HEADER_PATTERN)
                extracted_texts.append((os.path.basename(pdf), text))
            except Exception as e:
                logger.error(f"Failed to process text from {pdf}: {str(e)}")
                continue
       
        for future in as_completed(hyperlink_futures):
            pdf = hyperlink_futures[future]
            try:
                all_hyperlinks.extend(future.result())
            except Exception as e:
                logger.error(f"Failed to process hyperlinks from {pdf}: {str(e)}")
                continue

    if not extracted_texts:
        raise HTTPException(status_code=400, detail="No valid text extracted from PDFs")

    full_source_text = "\n".join(
        f"File: {name}\n{text}" for name, text in extracted_texts
    )

    max_attempts = 3
    attempt = 1
    score = 0
    while attempt <= max_attempts:
        logger.info(f"Starting report generation attempt {attempt}/{max_attempts}")
        
        # Reset shared_state.metrics to ensure fresh data
        with shared_state.lock:
            shared_state.metrics = None

        # Get sub-crews
        data_crew, report_crew, viz_crew = setup_crew(full_source_text, versions, llm)
   
        # Run data_crew
        logger.info("Starting data_crew")
        await data_crew.kickoff_async()
        logger.info("Data_crew completed")
   
        # Validate task outputs
        for i, task in enumerate(data_crew.tasks):
            if not hasattr(task, 'output') or not hasattr(task.output, 'raw'):
                logger.error(f"Invalid output for data_crew task {i}: {task}")
                raise ValueError(f"Data crew task {i} did not produce a valid output")
            logger.info(f"Data_crew task {i} output: {task.output.raw[:200]}...")

        # Validate metrics
        if not shared_state.metrics or not isinstance(shared_state.metrics, dict):
            logger.error(f"Invalid metrics in shared_state: type={type(shared_state.metrics)}, value={shared_state.metrics}")
            raise HTTPException(status_code=500, detail="Failed to generate valid metrics data")
        logger.info(f"Metrics after data_crew: {json.dumps(shared_state.metrics, indent=2)[:200]}...")

        # Run report_crew and viz_crew in parallel
        logger.info("Starting report_crew and viz_crew")
        await asyncio.gather(
            report_crew.kickoff_async(),
            viz_crew.kickoff_async()
        )
        logger.info("Report_crew and viz_crew completed")

        # Validate report_crew output
        if not hasattr(report_crew.tasks[-1], 'output') or not hasattr(report_crew.tasks[-1].output, 'raw'):
            logger.error(f"Invalid output for report_crew task {report_crew.tasks[-1]}")
            raise ValueError("Report crew did not produce a valid output")
        logger.info(f"Report_crew output: {report_crew.tasks[-1].output.raw[:100]}...")

        # Validate viz_crew output
        if not hasattr(viz_crew.tasks[0], 'output') or not hasattr(viz_crew.tasks[0].output, 'raw'):
            logger.error(f"Invalid output for viz_crew task {viz_crew.tasks[0]}")
            raise ValueError("Visualization crew did not produce a valid output")
        logger.info(f"Viz_crew output: {viz_crew.tasks[0].output.raw[:100]}...")

        metrics = shared_state.metrics

        # Get report from assemble_report_task
        enhanced_report = enhance_report_markdown(report_crew.tasks[-1].output.raw)
        if not validate_report(enhanced_report):
            logger.error("Report missing required sections")
            raise HTTPException(status_code=500, detail="Generated report is incomplete")

        viz_folder = "visualizations"
        if os.path.exists(viz_folder):
            shutil.rmtree(viz_folder)
        os.makedirs(viz_folder, exist_ok=True)

        script_path = "visualizations.py"
        raw_script = viz_crew.tasks[0].output.raw
        clean_script = re.sub(r'```python|```$', '', raw_script, flags=re.MULTILINE).strip()

        try:
            with shared_state.viz_lock:
                with open(script_path, "w", encoding="utf-8") as f:
                    f.write(clean_script)
                logger.info(f"Visualization script written to {script_path}")
                logger.debug(f"Visualization script content:\n{clean_script}")
                runpy.run_path(script_path, init_globals={'metrics': metrics})
                logger.info("Visualization script executed successfully")
        except Exception as e:
            logger.error(f"Visualization script failed: {str(e)}")
            logger.info("Running fallback visualization")
            run_fallback_visualization(metrics)

        viz_base64 = []
        expected_count = 10 + (1 if 'Pass/Fail' in metrics.get('metrics', {}) else 0)
        min_visualizations = 5
        if os.path.exists(viz_folder):
            viz_files = sorted([f for f in os.listdir(viz_folder) if f.endswith('.png')])
            for img in viz_files:
                img_path = os.path.join(viz_folder, img)
                base64_str = get_base64_image(img_path)
                if base64_str:
                    viz_base64.append(base64_str)
            logger.info(f"Generated {len(viz_base64)} visualizations, expected {expected_count}, minimum required {min_visualizations}")
            if len(viz_base64) < min_visualizations:
                logger.warning("Insufficient visualizations, running fallback")
                run_fallback_visualization(metrics)
                viz_files = sorted([f for f in os.listdir(viz_folder) if f.endswith('.png')])
                viz_base64 = []
                for img in viz_files:
                    img_path = os.path.join(viz_folder, img)
                    base64_str = get_base64_image(img_path)
                    if base64_str:
                        viz_base64.append(base64_str)
                if len(viz_base64) < min_visualizations:
                    logger.error(f"Still too few visualizations: {len(viz_base64)}")
                    raise HTTPException(
                        status_code=500,
                        detail=f"Failed to generate minimum required visualizations: got {len(viz_base64)}, need at least {min_visualizations}"
                    )

        score, evaluation = evaluate_with_llm_judge(full_source_text, enhanced_report)
        logger.info(f"Attempt {attempt}: Evaluation score = {score}")
        
        if score > 84:
            logger.info(f"Score {score} exceeds threshold of 84, proceeding with response")
            return AnalysisResponse(
                metrics=metrics,
                visualizations=viz_base64,
                report=enhanced_report,
                evaluation={"score": score, "text": evaluation},
                hyperlinks=all_hyperlinks
            )
        else:
            logger.warning(f"Score {score} <= 84, retrying report generation (attempt {attempt}/{max_attempts})")
            attempt += 1
            if attempt > max_attempts:
                logger.warning(f"Max attempts ({max_attempts}) reached with score {score}, proceeding with final response")
                return AnalysisResponse(
                    metrics=metrics,
                    visualizations=viz_base64,
                    report=enhanced_report,
                    evaluation={"score": score, "text": evaluation},
                    hyperlinks=all_hyperlinks
                )
